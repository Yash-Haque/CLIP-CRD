{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"clip_prefix_captioning_inference.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1eb939f6352f4063808350cc4a97beae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_56c9a56fb276438ab3f6b120193deb42","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_873ac85e2658404ea6b284707824a07d","IPY_MODEL_bcd00181175a4b739afc7a5eb30a235b","IPY_MODEL_2094ef62bd9547fcb133f565818495fa"]}},"56c9a56fb276438ab3f6b120193deb42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"873ac85e2658404ea6b284707824a07d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a2eb4632f5d477182fcbf0ebcd7eb5b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_93c09ca8d2ab40d381163b41c3bd345d"}},"bcd00181175a4b739afc7a5eb30a235b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f0585589cf7b49098b0905ed10d96ecf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":548118077,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":548118077,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90bddc0c0d164e93996d0340f45bb9b5"}},"2094ef62bd9547fcb133f565818495fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5dc2c242f1594631a573bdbe4ace5a97","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 523M/523M [00:20&lt;00:00, 25.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ab34ffb29124fe9a2bf8b2aadf8c514"}},"5a2eb4632f5d477182fcbf0ebcd7eb5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"93c09ca8d2ab40d381163b41c3bd345d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0585589cf7b49098b0905ed10d96ecf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"90bddc0c0d164e93996d0340f45bb9b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5dc2c242f1594631a573bdbe4ace5a97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7ab34ffb29124fe9a2bf8b2aadf8c514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6961430,"sourceType":"datasetVersion","datasetId":3999049},{"sourceId":6980167,"sourceType":"datasetVersion","datasetId":4011225},{"sourceId":7032863,"sourceType":"datasetVersion","datasetId":4045470},{"sourceId":7115034,"sourceType":"datasetVersion","datasetId":4103158},{"sourceId":7132674,"sourceType":"datasetVersion","datasetId":4115387},{"sourceId":7137213,"sourceType":"datasetVersion","datasetId":4118609}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Training CLIPCap on COCO 2014 Dataset:","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python parse_coco_yash.py --clip_model_type ViT-B/32","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile parse_coco_yash.py\nimport torch\nimport skimage.io as io\nimport clip\nfrom PIL import Image\nimport pickle\nimport json\nimport os\nfrom tqdm import tqdm\nimport argparse\n\n\ndef main(clip_model_type: str):\n    device = torch.device('cuda:0')\n    clip_model_name = clip_model_type.replace('/', '_')    \n    out_path = f\"/kaggle/working/data/coco/oscar_split_{clip_model_name}_train.pkl\"\n    \n    # Yash 04/12/23\n    out_folder = \"/kaggle/working/data/coco/\"\n    os.makedirs(out_folder, exist_ok=True)\n    print(\"\\n\\nMatter of output path resolved.\\n\\n\")\n    \n    clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n    with open('/kaggle/input/coco-training-captions/train_caption.json', 'r') as f:\n        data = json.load(f)\n    print(\"%0d captions loaded from json \" % len(data))\n    all_embeddings = []\n    all_captions = []\n    for i in tqdm(range(len(data))):\n        d = data[i]\n        img_id = d[\"image_id\"]\n        filename = f\"/kaggle/input/coco-dataset-2014/train2014/train2014/COCO_train2014_{int(img_id):012d}.jpg\"\n        if not os.path.isfile(filename):\n            filename = f\"/kaggle/input/coco-dataset-2014/val2014/val2014/COCO_val2014_{int(img_id):012d}.jpg\"\n        image = io.imread(filename)\n        image = preprocess(Image.fromarray(image)).unsqueeze(0).to(device)\n        with torch.no_grad():\n            prefix = clip_model.encode_image(image).cpu()\n        d[\"clip_embedding\"] = i\n        all_embeddings.append(prefix)\n        all_captions.append(d)\n        if (i + 1) % 10000 == 0:\n            with open(out_path, 'wb') as f:\n                pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\n    with open(out_path, 'wb') as f:\n        pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\n    print('Done')\n    print(\"%0d embeddings saved \" % len(all_embeddings))\n    return 0\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--clip_model_type', default=\"ViT-B/32\", choices=('RN50', 'RN101', 'RN50x4', 'ViT-B/32'))\n    args = parser.parse_args()\n    exit(main(args.clip_model_type))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py --data ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_yash.py --out_dir \"/kaggle/working/coco_train/\" --prefix_length 40 --prefix_length_clip 40 --mapping_type \"transformer\" --num_layers 8 ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_yash.py\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom enum import Enum\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\n\n# Yashfinul Haque 06/12/23\nimport logging\n\n\n\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\n\nclass ClipCocoDataset(Dataset):\n\n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        \n        logging.debug(f\"This is a debug message in 'pad_tokens'- function: {tokens},{mask}\")\n        \n        return tokens, mask\n\n    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[self.caption2embedding[item]]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        \n        logging.debug(f\"This is a debug message in 'get_item'- function: {tokens},{mask}, {prefix}\")\n        return tokens, mask, prefix\n\n    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n        self.captions = [caption['caption'] for caption in captions_raw]\n        \n        # Yashfinul Haque 06/12/23\n        os.makedirs(os.path.dirname(f\"{data_path[:-4]}_tokens.pkl\"), exist_ok=True)\n        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n        else:\n            self.captions_tokens = []\n            self.caption2embedding = []\n            max_seq_len = 0\n            for caption in captions_raw:\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n                self.caption2embedding.append(caption[\"clip_embedding\"])\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n            # self.max_seq_len = max_seq_len\n            \n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n\nclass MLP(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass MlpTransformer(nn.Module):\n    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n        super().__init__()\n        out_d = out_d if out_d is not None else in_dim\n        self.fc1 = nn.Linear(in_dim, h_dim)\n        self.act = act\n        self.fc2 = nn.Linear(h_dim, out_d)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention\n\n\nclass TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)\n\n\nclass TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                     self.gpt_embedding_size * prefix_length))\n        else:\n            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n                                                                     clip_length, num_layers)\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n\n\ndef save_config(args: argparse.Namespace):\n    config = {}\n    for key, item in args._get_kwargs():\n        config[key] = item\n    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n    with open(out_path, 'w') as outfile:\n        json.dump(config, outfile)\n\n\ndef load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n    with open(config_path) as f:\n        config = json.load(f)\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(**config)\n    args = parser.parse_args()\n    if type(epoch_or_latest) is int:\n        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n    if args.only_prefix:\n        model = ClipCaptionPrefix(args.prefix_length)\n    else:\n        model = ClipCaptionModel(args.prefix_length)\n    if os.path.isfile(model_path):\n        print(f\"loading model from {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n    else:\n        print(f\"{model_path} is not exist\")\n    return model, parser\n\n\ndef train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n\n    device = torch.device('cuda:0')\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = model.to(device)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    # save_config(args)\n    for epoch in range(epochs):\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss\": loss.item()})\n            progress.update()\n            if (idx + 1) % 10000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0 or epoch == epochs - 1:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n            )\n    return model\n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    logging.getLogger().setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', default='/kaggle/input/oscar-split-vit-b-32-train/oscar_split_ViT-B_32_train.pkl')\n    parser.add_argument('--out_dir', default='/kaggle/working/checkpoints')\n    parser.add_argument('--prefix', default='coco_prefix', help='prefix for saved filenames')\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--save_every', type=int, default=1)\n    parser.add_argument('--prefix_length', type=int, default=10)\n    parser.add_argument('--prefix_length_clip', type=int, default=10)\n    parser.add_argument('--bs', type=int, default=40)\n    parser.add_argument('--only_prefix', dest='only_prefix', action='store_true')\n    parser.add_argument('--mapping_type', type=str, default='mlp', help='mlp/transformer')\n    parser.add_argument('--num_layers', type=int, default=8)\n    parser.add_argument('--is_rn', dest='is_rn', action='store_true')\n    parser.add_argument('--normalize_prefix', dest='normalize_prefix', action='store_true')\n    args = parser.parse_args()\n    prefix_length = args.prefix_length\n    dataset = ClipCocoDataset(args.data, prefix_length, normalize_prefix=args.normalize_prefix)\n    prefix_dim = 640 if args.is_rn else 512\n    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]\n    if args.only_prefix:\n        model = ClipCaptionPrefix(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n        print(\"Train only prefix\")\n    else:\n        model = ClipCaptionModel(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n        print(\"Train both prefix and GPT\")\n        sys.stdout.flush()\n    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Custom Dataset","metadata":{}},{"cell_type":"markdown","source":"### **Viewing The Keys of the Dataset**","metadata":{}},{"cell_type":"code","source":"import json\n\ntrain_file_path = \"/kaggle/input/open-i-dataset/Open-I Dataset/Captions/Train.jsonl\"\nwith open(train_file_path, 'r', encoding='utf-8-sig') as file:\n    data = file.read()\n\nresult = [json.loads(json_line) for json_line in data.splitlines()]\n\nprint(len(result)) # important\n#print(result[1])\ni = 0\nd = result[i]\n\nimg_id = d[\"id\"]\nprint(f\"Real URL: {d['img']}\")\nprint(f\"'{img_id}'\")\nfilename = f\"/kaggle/input/open-i-dataset/Open-I Dataset/images/{int(img_id)}.jpg\"\nprint(f\"Filename: {filename}\")\nprint(f\"{result[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:41:22.324918Z","iopub.execute_input":"2024-01-02T08:41:22.325902Z","iopub.status.idle":"2024-01-02T08:41:22.366021Z","shell.execute_reply.started":"2024-01-02T08:41:22.325866Z","shell.execute_reply":"2024-01-02T08:41:22.365135Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Modifying the Image Paths**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\ntrain_file_path = \"/kaggle/input/open-i-dataset/Open-I Dataset/Captions/Train.jsonl\"\nnew_image_path = \"/kaggle/input/open-i-dataset/Open-I Dataset/images/\"\nwith open(train_file_path, 'r', encoding='utf-8-sig') as file:\n    jsonl_content = file.read()\n\nresult = [json.loads(jline) for jline in jsonl_content.splitlines()]\n\nmodified_lines = []\n\nfor entry in result:\n    index = entry['id']\n    #print(f\"Index Found:{index}\")\n    entry['img'] = f\"{new_image_path}/{index}.jpg\"\n    #print(f\"Modified Entry:{entry}\")\n    # Append the modified entry to the list\n    modified_lines.append(json.dumps(entry))\n \ni = 0\nfor i in range(5):\n    print(modified_lines[i])\n\n# Output path\nmodified_output = \"/kaggle/working/open-i-dataset/Open-I Dataset/Modified-Captions\"\n\n# Ensure the folder exists or is created\nos.makedirs(modified_output, exist_ok=True)\n\n# Write into the folder\nwith open(f\"{modified_output}/Train.jsonl\", 'w') as output_file:\n    output_file.write('\\n'.join(modified_lines))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:41:42.719239Z","iopub.execute_input":"2024-01-02T08:41:42.719617Z","iopub.status.idle":"2024-01-02T08:41:42.769156Z","shell.execute_reply.started":"2024-01-02T08:41:42.719588Z","shell.execute_reply":"2024-01-02T08:41:42.768282Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Determining the Text Lengths For Training**","metadata":{}},{"cell_type":"code","source":"import math\n\ntrain_file_path = \"/kaggle/working/open-i-dataset/Open-I Dataset/Modified-Captions/Train.jsonl\"\n\nwith open(train_file_path, 'r', encoding='utf-8-sig') as file:\n    data = file.read()\n\nresult = [json.loads(json_line) for json_line in data.splitlines()]\n\nmax_length = 0\n\nstrings = []\n\nfor entry in result:\n    strings_list = entry.get('text', [])\n    strings.append(strings_list)\n    \nmax_length = max(len(string) for string in strings) if strings else 0\naverage_length = sum(len(string) for string in strings) / len(strings) if strings else 0\naverage_length = math.ceil(average_length)\n\nprint(f\"Maximum length of strings for key 'Text': {max_length}\")\nprint(f\"Average length of strings for key 'Text': {average_length}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:41:45.300209Z","iopub.execute_input":"2024-01-02T08:41:45.300818Z","iopub.status.idle":"2024-01-02T08:41:45.327452Z","shell.execute_reply.started":"2024-01-02T08:41:45.300786Z","shell.execute_reply":"2024-01-02T08:41:45.326482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Parsing Open-I Dataset to create \"Image-Caption\" Pair**","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:41:47.640136Z","iopub.execute_input":"2024-01-02T08:41:47.640978Z","iopub.status.idle":"2024-01-02T08:41:47.646966Z","shell.execute_reply.started":"2024-01-02T08:41:47.640940Z","shell.execute_reply":"2024-01-02T08:41:47.646017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:41:48.900013Z","iopub.execute_input":"2024-01-02T08:41:48.900682Z","iopub.status.idle":"2024-01-02T08:42:04.948467Z","shell.execute_reply.started":"2024-01-02T08:41:48.900649Z","shell.execute_reply":"2024-01-02T08:42:04.947334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-Image","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:20:46.020528Z","iopub.execute_input":"2024-01-02T08:20:46.020904Z","iopub.status.idle":"2024-01-02T08:20:57.402435Z","shell.execute_reply.started":"2024-01-02T08:20:46.020871Z","shell.execute_reply":"2024-01-02T08:20:57.401433Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile parse_openi_yash.py\nimport torch\nimport skimage.io as io\nimport clip\nfrom PIL import Image\nimport pickle\nimport json\nimport os\nfrom tqdm import tqdm\nimport argparse\n\n\ndef main(clip_model_type: str):\n    device = torch.device('cuda:0')\n    clip_model_name = clip_model_type.replace('/', '_')    \n    out_path = f\"/kaggle/working/data/open-i/oscar_split_{clip_model_name}_train.pkl\"\n    \n    # Yash 07/12/23\n    out_folder = \"/kaggle/working/data/open-i/\"\n    train_file_path = \"/kaggle/working/open-i-dataset/Open-I Dataset/Modified-Captions/Train.jsonl\"\n    os.makedirs(out_folder, exist_ok=True)\n    print(\"\\n\\nMatter of output path resolved.\\n\\n\")\n    \n    clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n    with open(train_file_path, 'r', encoding='utf-8-sig') as file:\n        data = file.read()\n    result = [json.loads(json_line) for json_line in data.splitlines()]\n    print(\"\\n%0d captions loaded from json \\n\" % len(result))\n    \n    all_embeddings = []\n    all_captions = []\n#     captions = {}\n    i = 0\n    for i in tqdm(range(len(result))):\n        d = result[i]\n        img_id: int = int(d[\"id\"])\n        caption: str = d['text']     \n        filename = f\"/kaggle/input/open-i-dataset/Open-I Dataset/images/{img_id}.jpg\"\n        \n        # Image Preprocessing\n        image = io.imread(filename)\n        image = preprocess(Image.fromarray(image)).unsqueeze(0).to(device)\n        \n        # Image Encoding\n        with torch.no_grad():\n            prefix = clip_model.encode_image(image).cpu()\n        d[\"clip_embedding\"] = i\n        \n        # Combining all image embeddings\n        all_embeddings.append(prefix)\n        \n        # Combining all caption\n        all_captions.append(d)\n        \n        # Wrapping 'id',img_id','caption' into a single dictionary called 'captions'\n    \n#         captions['id'] = i\n#         captions['image_id'] = img_id\n#         captions['caption'] = caption\n        \n        \n        # Periodic saving of embeddings at every 10k iterations for better memory management.\n        if (i + 1) % 10000 == 0:\n            with open(out_path, 'wb') as f:\n                pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\n    with open(out_path, 'wb') as f:\n#         print(f\"id data type: {type(all_ids)}\")\n#         print(f\"all_img_ids data type: {type(all_img_ids[i])}\")\n#         print(f\"clip_embedding data type: {type(all_img_ids[i])}\")\n#         print(f\"captions data type: {type(all_img_ids[i])}\")\n        \n         \n        \n        pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n#     dict = {\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}\n#     print(f\"An example of the new file: {d}\")\n#     print(\"Example of the file just saved:\")\n#     print(f\"\\nClip_Embedding: {dict.get('clip_embedding',[])}, Captions: {dict.get('captions',[])}\\n\")\n#    print(f\"\\nSaved Dictionary: {dict[0:5]}\\n\")\n    print('Done')\n    print(\"%0d embeddings saved \" % len(all_embeddings))\n    return 0\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--clip_model_type', default=\"ViT-B/32\", choices=('RN50', 'RN101', 'RN50x4', 'ViT-B/32'))\n    args = parser.parse_args()\n    exit(main(args.clip_model_type))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:42:06.566316Z","iopub.execute_input":"2024-01-02T08:42:06.567192Z","iopub.status.idle":"2024-01-02T08:42:06.576640Z","shell.execute_reply.started":"2024-01-02T08:42:06.567155Z","shell.execute_reply":"2024-01-02T08:42:06.575661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python parse_openi_yash.py --clip_model_type ViT-B/32","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:42:09.836737Z","iopub.execute_input":"2024-01-02T08:42:09.837729Z","iopub.status.idle":"2024-01-02T08:43:21.960181Z","shell.execute_reply.started":"2024-01-02T08:42:09.837692Z","shell.execute_reply":"2024-01-02T08:43:21.959178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Analyzing the parsed pickle file**","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nfrom transformers import GPT2Tokenizer\nfrom tqdm import tqdm\n\n# Specify the path to your pickle file\nfile_path = \"/kaggle/working/data/open-i/oscar_split_ViT-B_32_train.pkl\"  # Replace with the actual file path\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# Open the pickle file in read-binary mode\nwith open(file_path, \"rb\") as f:\n    # Load the data from the file using pickle.load\n    all_data = pickle.load(f)\n\nprint(all_data.keys())\n\n\n#id = []\n#clip_embedding = []\n#captions = []\n\nid = all_data['img_id']\nprefixes = all_data['clip_embedding']\ncaptions_raw = all_data['captions']\n\nimage_ids = []   \ncaption_tokens = []\nclip2embedding = []\n\n#image_ids = [int(img_id) for img_id in all_data['img_id']]\nmax_seq_len = i = 0\nfor i in tqdm(range(len(captions_raw))):\n    caption_tokens.append(torch.tensor(tokenizer.encode(captions_raw[i]), dtype=torch.int64))\n    clip2embedding.append(prefixes[i])\n    max_seq_len = max(max_seq_len, caption_tokens[-1].shape[0])\n\nflat_caption_tokens = [token for tokens in caption_tokens for token in tokens]  # Flatten the list\nwords = tokenizer.decode(flat_caption_tokens, skip_special_tokens=True)\n# print(f\"Caption_tokens: {words}\")\n\n# Second implementation from chatGPT\ni = 0\nwords = []\nfor i in range(len(caption_tokens)):\n    tokens = caption_tokens[i]\n    words.append(tokenizer.decode(tokens, skip_special_tokens=True))\n\n    print(f\"\\nCaption_tokens: {words}\\n\")\n\n\nprint(f\"Max Sequence Length: {max_seq_len}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Wrapping** the Open-I Dataset into a captions dictionary","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\n# Set PYTORCH_CUDA_ALLOC_CONF environment variable\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\"\n\n# Explanation: By setting PYTORCH_CUDA_ALLOC_CONF to \"caching_allocator\",\n# we enable the caching memory allocator, which improves memory management efficiency.\n\n# Create a CUDA tensor\nx = torch.randn(1000, 1000).cuda()\n\n# Explanation: Here, we create a CUDA tensor using the torch.randn() function.\n# Since PYTORCH_CUDA_ALLOC_CONF is set, the tensor will be allocated using the caching allocator.\n\n# Perform some computations\ny = x + x.t()\nz = torch.matmul(y, y)\n\n# Explanation: We perform some computations on the CUDA tensor.\n# The caching allocator manages the memory allocation and reuse efficiently,\n# reducing the overhead of memory allocation and deallocation operations.\n\n# Clear memory explicitly (optional)\ndel x, y, z\n\n# Explanation: Clearing the variables is optional, but it can help release GPU memory\n# before subsequent operations to avoid excessive memory usage.\n\n# Reset PYTORCH_CUDA_ALLOC_CONF environment variable (optional)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"\"\n\n# Explanation: Resetting PYTORCH_CUDA_ALLOC_CONF to an empty string restores\n# the default memory allocator behavior in PyTorch.\n\n# Continue with other operations","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Train Open-I Dataset**","metadata":{}},{"cell_type":"code","source":"%%writefile train_yash.py\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom enum import Enum\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\n\n# Yashfinul Haque 06/12/23\nimport logging\n\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\n          \n# torch.cuda.empty_cache()\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\"\n\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\n\nclass ClipCocoDataset(Dataset):\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        # print(f\"\\nTokens: {tokens}\\n\")\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        \n        logging.debug(f\"This is a debug message in 'pad_tokens'- function: {tokens},{mask}\")\n        \n        return tokens, mask\n\n    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[self.caption2embedding[item]]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        \n        logging.debug(f\"This is a debug message in 'get_item'- function: {tokens},{mask}, {prefix}\")\n        return tokens, mask, prefix\n\n    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        print(f\"\\nData Path: {data_path}\\n\")\n        # Yashfinul Haque - 30/12/23 - Wrapped the extracting of the data in a try exception block.\n        \n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n\n        #print(f\"Data after loading:{all_data}\")\n        print(\"Data size is %0d\" % len(all_data['clip_embedding']))\n        sys.stdout.flush()\n            \n            \n        self.prefixes = all_data['clip_embedding']\n        captions_raw = all_data['captions']\n        self.image_ids = [caption[\"id\"] for caption in captions_raw]\n        self.captions = [caption['text'] for caption in captions_raw]\n        print(f\"\\nPrefix Type: {type(self.prefixes)}\\n\")\n        print(f\"\\nCaptions Type: {type(captions_raw)}\\n\")\n        print(f\"\\nPrefix: {self.prefixes}\\n\")\n        \n        \n        # Yashfinul Haque 06/12/23\n        os.makedirs(os.path.dirname(f\"{data_path[:-4]}_tokens.pkl\"), exist_ok=True)\n        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n        else:\n            self.captions_tokens = []\n            self.caption2embedding = []\n        \n        # Original\n            max_seq_len = 0\n            for caption in captions_raw:\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['text']), dtype=torch.int64))\n                self.caption2embedding.append(caption[\"clip_embedding\"])\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n            \n            # self.max_seq_len = max_seq_len\n            \n        \n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n            \n        \n        \nclass MLP(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass MlpTransformer(nn.Module):\n    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n        super().__init__()\n        out_d = out_d if out_d is not None else in_dim\n        self.fc1 = nn.Linear(in_dim, h_dim)\n        self.act = act\n        self.fc2 = nn.Linear(h_dim, out_d)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention\n\n\nclass TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)\n\n\nclass TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                     self.gpt_embedding_size * prefix_length))\n        else:\n            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n                                                                     clip_length, num_layers)\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n\n\ndef save_config(args: argparse.Namespace):\n    config = {}\n    for key, item in args._get_kwargs():\n        config[key] = item\n    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n    with open(out_path, 'w') as outfile:\n        json.dump(config, outfile)\n\n\ndef load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n    with open(config_path) as f:\n        config = json.load(f)\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(**config)\n    args = parser.parse_args()\n    if type(epoch_or_latest) is int:\n        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n    if args.only_prefix:\n        model = ClipCaptionPrefix(args.prefix_length)\n    else:\n        model = ClipCaptionModel(args.prefix_length)\n    if os.path.isfile(model_path):\n        print(f\"loading model from {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n    else:\n        print(f\"{model_path} is not exist\")\n    return model, parser\n\n\ndef train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n\n    device = torch.device('cuda:0')\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = model.to(device)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    # save_config(args)\n    for epoch in range(epochs):\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss\": loss.item()})\n            progress.update()\n            if (idx + 1) % 10000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                )\n        progress.close()\n        if epoch % args.save_every == 0 or epoch == epochs - 1:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n            )\n    return model\n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    logging.getLogger().setLevel(logging.DEBUG)\n    free_gpu_cache()\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', default=\"/kaggle/working/data/open-i/oscar_split_ViT-B_32_train.pkl\")\n    parser.add_argument('--out_dir', default='/kaggle/working/checkpoints')\n    parser.add_argument('--prefix', default='coco_prefix', help='prefix for saved filenames')\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--save_every', type=int, default=1)\n    parser.add_argument('--prefix_length', type=int, default=10)\n    parser.add_argument('--prefix_length_clip', type=int, default=10)\n    parser.add_argument('--bs', type=int, default=40)\n    parser.add_argument('--only_prefix', dest='only_prefix', action='store_true')\n    parser.add_argument('--mapping_type', type=str, default='mlp', help='mlp/transformer')\n    parser.add_argument('--num_layers', type=int, default=8)\n    parser.add_argument('--is_rn', dest='is_rn', action='store_true')\n    parser.add_argument('--normalize_prefix', dest='normalize_prefix', action='store_true')\n    args = parser.parse_args()\n    prefix_length = args.prefix_length\n    dataset = ClipCocoDataset(args.data, prefix_length, normalize_prefix=args.normalize_prefix)\n    prefix_dim = 640 if args.is_rn else 512\n    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]\n    if args.only_prefix:\n        model = ClipCaptionPrefix(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n        print(\"Train only prefix\")\n    else:\n        model = ClipCaptionModel(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n        print(\"Train both prefix and GPT\")\n        sys.stdout.flush()\n    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:44:28.753106Z","iopub.execute_input":"2024-01-02T08:44:28.753559Z","iopub.status.idle":"2024-01-02T08:44:28.770717Z","shell.execute_reply.started":"2024-01-02T08:44:28.753526Z","shell.execute_reply":"2024-01-02T08:44:28.769743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install GPUtil","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:44:32.457637Z","iopub.execute_input":"2024-01-02T08:44:32.458487Z","iopub.status.idle":"2024-01-02T08:44:34.831426Z","shell.execute_reply.started":"2024-01-02T08:44:32.458447Z","shell.execute_reply":"2024-01-02T08:44:34.830431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_yash.py --out_dir \"/kaggle/working/openi_train/\" --prefix_length 250 --prefix_length_clip 250 --mapping_type \"transformer\" --num_layers 8 ","metadata":{"execution":{"iopub.status.busy":"2024-01-02T08:44:37.460058Z","iopub.execute_input":"2024-01-02T08:44:37.460419Z","iopub.status.idle":"2024-01-02T08:44:56.093941Z","shell.execute_reply.started":"2024-01-02T08:44:37.460392Z","shell.execute_reply":"2024-01-02T08:44:56.092948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Eval Script","metadata":{}},{"cell_type":"code","source":"# Configuration for Cog ⚙️\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nimage: r8.im/rmokady/clip_prefix_caption\n\nbuild:\n  # set to true if your model requires a GPU\n  gpu: true\n  cuda: \"10.2\"\n\n  # a list of ubuntu apt packages to install\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n\n  # python version in the form '3.8' or '3.8.12'\n  python_version: \"3.8\"\n\n  # a list of packages in the format <package-name>==<version>\n  python_packages:\n    - \"transformers==4.11.3\"\n    - \"git+https://github.com/openai/CLIP.git\"\n    - \"torch==1.9.1\"\n    - \"numpy==1.19.5\"\n    - \"pillow==8.3.2\"\n    - \"scikit-image==0.16.2\"\n\n  # commands run after the enviroment is setup\n  run:\n    # - \"echo env is ready!\"\n    # - \"echo another command if needed\"\n\n# predict.py defines how predictions are run on your model\npredict: \"predict.py:Predictor\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction interface for Cog ⚙️\n# Reference: https://github.com/replicate/cog/blob/main/docs/python.md\n\nimport clip\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch\nimport torch.nn.functional as nnf\nimport sys\nfrom typing import Tuple, List, Union, Optional\nfrom transformers import (\n    GPT2Tokenizer,\n    GPT2LMHeadModel,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nimport skimage.io as io\nimport PIL.Image\n\nimport cog\n\n# import torch\n\nN = type(None)\nV = np.array\nARRAY = np.ndarray\nARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\nVS = Union[Tuple[V, ...], List[V]]\nVN = Union[V, N]\nVNS = Union[VS, N]\nT = torch.Tensor\nTS = Union[Tuple[T, ...], List[T]]\nTN = Optional[T]\nTNS = Union[Tuple[TN, ...], List[TN]]\nTSN = Optional[TS]\nTA = Union[T, ARRAY]\n\nWEIGHTS_PATHS = {\n    \"coco\": \"coco_weights.pt\",\n    \"conceptual-captions\": \"conceptual_weights.pt\",\n}\n\nD = torch.device\nCPU = torch.device(\"cpu\")\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        self.device = torch.device(\"cuda\")\n        self.clip_model, self.preprocess = clip.load(\n            \"ViT-B/32\", device=self.device, jit=False\n        )\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n        self.models = {}\n        self.prefix_length = 10\n        for key, weights_path in WEIGHTS_PATHS.items():\n            model = ClipCaptionModel(self.prefix_length)\n            model.load_state_dict(torch.load(weights_path, map_location=CPU))\n            model = model.eval()\n            model = model.to(self.device)\n            self.models[key] = model\n\n    @cog.input(\"image\", type=cog.Path, help=\"Input image\")\n    @cog.input(\n        \"model\",\n        type=str,\n        options=WEIGHTS_PATHS.keys(),\n        default=\"coco\",\n        help=\"Model to use\",\n    )\n    @cog.input(\n        \"use_beam_search\",\n        type=bool,\n        default=False,\n        help=\"Whether to apply beam search to generate the output text\",\n    )\n    def predict(self, image, model, use_beam_search):\n        \"\"\"Run a single prediction on the model\"\"\"\n        image = io.imread(image)\n        model = self.models[model]\n        pil_image = PIL.Image.fromarray(image)\n        image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            prefix = self.clip_model.encode_image(image).to(\n                self.device, dtype=torch.float32\n            )\n            prefix_embed = model.clip_project(prefix).reshape(1, self.prefix_length, -1)\n        if use_beam_search:\n            return generate_beam(model, self.tokenizer, embed=prefix_embed)[0]\n        else:\n            return generate2(model, self.tokenizer, embed=prefix_embed)\n\n\nclass MLP(nn.Module):\n    def forward(self, x: T) -> T:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    # @functools.lru_cache #FIXME\n    def get_dummy_token(self, batch_size: int, device: D) -> T:\n        return torch.zeros(\n            batch_size, self.prefix_length, dtype=torch.int64, device=device\n        )\n\n    def forward(\n        self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None\n    ):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(\n            -1, self.prefix_length, self.gpt_embedding_size\n        )\n        # print(embedding_text.size()) #torch.Size([5, 67, 768])\n        # print(prefix_projections.size()) #torch.Size([5, 1, 768])\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, prefix_size: int = 512):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if prefix_length > 10:  # not enough memory\n            self.clip_project = nn.Linear(\n                prefix_size, self.gpt_embedding_size * prefix_length\n            )\n        else:\n            self.clip_project = MLP(\n                (\n                    prefix_size,\n                    (self.gpt_embedding_size * prefix_length) // 2,\n                    self.gpt_embedding_size * prefix_length,\n                )\n            )\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n\n\ndef generate_beam(\n    model,\n    tokenizer,\n    beam_size: int = 5,\n    prompt=None,\n    embed=None,\n    entry_length=67,\n    temperature=1.0,\n    stop_token: str = \".\",\n):\n\n    model.eval()\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    tokens = None\n    scores = None\n    device = next(model.parameters()).device\n    seq_lengths = torch.ones(beam_size, device=device)\n    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n    with torch.no_grad():\n        if embed is not None:\n            generated = embed\n        else:\n            if tokens is None:\n                tokens = torch.tensor(tokenizer.encode(prompt))\n                tokens = tokens.unsqueeze(0).to(device)\n                generated = model.gpt.transformer.wte(tokens)\n        for i in range(entry_length):\n            outputs = model.gpt(inputs_embeds=generated)\n            logits = outputs.logits\n            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n            logits = logits.softmax(-1).log()\n            if scores is None:\n                scores, next_tokens = logits.topk(beam_size, -1)\n                generated = generated.expand(beam_size, *generated.shape[1:])\n                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n                if tokens is None:\n                    tokens = next_tokens\n                else:\n                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n                    tokens = torch.cat((tokens, next_tokens), dim=1)\n            else:\n                logits[is_stopped] = -float(np.inf)\n                logits[is_stopped, 0] = 0\n                scores_sum = scores[:, None] + logits\n                seq_lengths[~is_stopped] += 1\n                scores_sum_average = scores_sum / seq_lengths[:, None]\n                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n                    beam_size, -1\n                )\n                next_tokens_source = next_tokens // scores_sum.shape[1]\n                seq_lengths = seq_lengths[next_tokens_source]\n                next_tokens = next_tokens % scores_sum.shape[1]\n                next_tokens = next_tokens.unsqueeze(1)\n                tokens = tokens[next_tokens_source]\n                tokens = torch.cat((tokens, next_tokens), dim=1)\n                generated = generated[next_tokens_source]\n                scores = scores_sum_average * seq_lengths\n                is_stopped = is_stopped[next_tokens_source]\n            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(\n                generated.shape[0], 1, -1\n            )\n            generated = torch.cat((generated, next_token_embed), dim=1)\n            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n            if is_stopped.all():\n                break\n    scores = scores / seq_lengths\n    output_list = tokens.cpu().numpy()\n    output_texts = [\n        tokenizer.decode(output[: int(length)])\n        for output, length in zip(output_list, seq_lengths)\n    ]\n    order = scores.argsort(descending=True)\n    output_texts = [output_texts[i] for i in order]\n    return output_texts\n\n\ndef generate2(\n    model,\n    tokenizer,\n    tokens=None,\n    prompt=None,\n    embed=None,\n    entry_count=1,\n    entry_length=67,  # maximum number of words\n    top_p=0.8,\n    temperature=1.0,\n    stop_token: str = \".\",\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    filter_value = -float(\"Inf\")\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n\n        for entry_idx in range(entry_count):\n            if embed is not None:\n                generated = embed\n            else:\n                if tokens is None:\n                    tokens = torch.tensor(tokenizer.encode(prompt))\n                    tokens = tokens.unsqueeze(0).to(device)\n\n                generated = model.gpt.transformer.wte(tokens)\n\n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(\n                    nnf.softmax(sorted_logits, dim=-1), dim=-1\n                )\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n                next_token = torch.argmax(logits, -1).unsqueeze(0)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n            output_text = tokenizer.decode(output_list)\n            generated_list.append(output_text)\n\n    return generated_list[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference notebook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)\n\nDisclaimer: the authors do not own any rights for the code or data.","metadata":{"id":"sdBjRnWqLwWP"}},{"cell_type":"code","source":"#@title Install\n!pip install transformers\n! pip install git+https://github.com/openai/CLIP.git\n","metadata":{"id":"GRfpGaz27IWs","outputId":"ebf43909-76e1-4c4a-e387-3501e9df9c4c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Drive Downloader\n\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\ndownload_with_pydrive = True #@param {type:\"boolean\"}  \n\nclass Downloader(object):\n    def __init__(self, use_pydrive):\n        self.use_pydrive = use_pydrive\n\n        if self.use_pydrive:\n            self.authenticate()\n        \n    def authenticate(self):\n        auth.authenticate_user()\n        gauth = GoogleAuth()\n        gauth.credentials = GoogleCredentials.get_application_default()\n        self.drive = GoogleDrive(gauth)\n    \n    def download_file(self, file_id, file_dst):\n        if self.use_pydrive:\n            downloaded = self.drive.CreateFile({'id':file_id})\n            downloaded.FetchMetadata(fetch_all=True)\n            downloaded.GetContentFile(file_dst)\n        else:\n            !gdown --id $file_id -O $file_dst\n\ndownloader = Downloader(download_with_pydrive)","metadata":{"id":"iqE3Fj5-uYSR","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Imports\n\nimport clip\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch\nimport torch.nn.functional as nnf\nimport sys\nfrom typing import Tuple, List, Union, Optional\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\n# from google.colab import files\nimport skimage.io as io\nimport PIL.Image\nfrom IPython.display import Image \n\n\nN = type(None)\nV = np.array\nARRAY = np.ndarray\nARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\nVS = Union[Tuple[V, ...], List[V]]\nVN = Union[V, N]\nVNS = Union[VS, N]\nT = torch.Tensor\nTS = Union[Tuple[T, ...], List[T]]\nTN = Optional[T]\nTNS = Union[Tuple[TN, ...], List[TN]]\nTSN = Optional[TS]\nTA = Union[T, ARRAY]\n\n\nD = torch.device\nCPU = torch.device('cpu')\n\n\ndef get_device(device_id: int) -> D:\n    if not torch.cuda.is_available():\n        return CPU\n    device_id = min(torch.cuda.device_count() - 1, device_id)\n    return torch.device(f'cuda:{device_id}')\n\n\nCUDA = get_device\n\ncurrent_directory = os.getcwd()\nsave_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\nos.makedirs(save_path, exist_ok=True)\nmodel_path = os.path.join(save_path, 'model_wieghts.pt')\n","metadata":{"id":"OArDkm_24w4L","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Model\n\nclass MLP(nn.Module):\n\n    def forward(self, x: T) -> T:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) -1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    #@functools.lru_cache #FIXME\n    def get_dummy_token(self, batch_size: int, device: D) -> T:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, prefix_size: int = 512):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if prefix_length > 10:  # not enough memory\n            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n        else:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"id":"4ClW2ebek8DK","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Caption prediction\n\ndef generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n                  entry_length=67, temperature=1., stop_token: str = '.'):\n\n    model.eval()\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    tokens = None\n    scores = None\n    device = next(model.parameters()).device\n    seq_lengths = torch.ones(beam_size, device=device)\n    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n    with torch.no_grad():\n        if embed is not None:\n            generated = embed\n        else:\n            if tokens is None:\n                tokens = torch.tensor(tokenizer.encode(prompt))\n                tokens = tokens.unsqueeze(0).to(device)\n                generated = model.gpt.transformer.wte(tokens)\n        for i in range(entry_length):\n            outputs = model.gpt(inputs_embeds=generated)\n            logits = outputs.logits\n            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n            logits = logits.softmax(-1).log()\n            if scores is None:\n                scores, next_tokens = logits.topk(beam_size, -1)\n                generated = generated.expand(beam_size, *generated.shape[1:])\n                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n                if tokens is None:\n                    tokens = next_tokens\n                else:\n                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n                    tokens = torch.cat((tokens, next_tokens), dim=1)\n            else:\n                logits[is_stopped] = -float(np.inf)\n                logits[is_stopped, 0] = 0\n                scores_sum = scores[:, None] + logits\n                seq_lengths[~is_stopped] += 1\n                scores_sum_average = scores_sum / seq_lengths[:, None]\n                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n                next_tokens_source = next_tokens // scores_sum.shape[1]\n                seq_lengths = seq_lengths[next_tokens_source]\n                next_tokens = next_tokens % scores_sum.shape[1]\n                next_tokens = next_tokens.unsqueeze(1)\n                tokens = tokens[next_tokens_source]\n                tokens = torch.cat((tokens, next_tokens), dim=1)\n                generated = generated[next_tokens_source]\n                scores = scores_sum_average * seq_lengths\n                is_stopped = is_stopped[next_tokens_source]\n            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n            generated = torch.cat((generated, next_token_embed), dim=1)\n            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n            if is_stopped.all():\n                break\n    scores = scores / seq_lengths\n    output_list = tokens.cpu().numpy()\n    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n    order = scores.argsort(descending=True)\n    output_texts = [output_texts[i] for i in order]\n    return output_texts\n\n\ndef generate2(\n        model,\n        tokenizer,\n        tokens=None,\n        prompt=None,\n        embed=None,\n        entry_count=1,\n        entry_length=67,  # maximum number of words\n        top_p=0.8,\n        temperature=1.,\n        stop_token: str = '.',\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    filter_value = -float(\"Inf\")\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n            if embed is not None:\n                generated = embed\n            else:\n                if tokens is None:\n                    tokens = torch.tensor(tokenizer.encode(prompt))\n                    tokens = tokens.unsqueeze(0).to(device)\n\n                generated = model.gpt.transformer.wte(tokens)\n\n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                                                    ..., :-1\n                                                    ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n                next_token = torch.argmax(logits, -1).unsqueeze(0)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n            output_text = tokenizer.decode(output_list)\n            generated_list.append(output_text)\n\n    return generated_list[0]","metadata":{"id":"V7xocT3TUgey","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Choose pretrained model - COCO or Coneptual captions\n\n\npretrained_model = 'Conceptual captions'  # @param ['COCO', 'Conceptual captions']\n\nif pretrained_model == 'Conceptual captions':\n  downloader.download_file(\"14pXWwB4Zm82rsDdvbGguLfx9F8aM7ovT\", model_path)\nelse:\n  downloader.download_file(\"1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX\", model_path)","metadata":{"id":"xE-uUStuv1Nl","cellView":"form"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title GPU/CPU\n\n\nis_gpu = True #@param {type:\"boolean\"}  \n","metadata":{"cellView":"form","id":"7lCgFHSgr_ny","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title CLIP model + GPT2 tokenizer\n\ndevice = CUDA(0) if is_gpu else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"id":"6bi_2zQ3QD57","cellView":"form","outputId":"47afad0d-a76c-4316-8f6d-682dd3e49587","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Load model weights\n\n\nprefix_length = 10\n\nmodel = ClipCaptionModel(prefix_length)\n\nmodel.load_state_dict(torch.load(model_path, map_location=CPU)) \n\nmodel = model.eval() \ndevice = CUDA(0) if is_gpu else \"cpu\"\nmodel = model.to(device)\n","metadata":{"id":"glBzYsgIwhwF","cellView":"form","outputId":"2d7637a4-1c76-44b1-c0aa-80a308ee0717","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Upload Image\n\n\nuploaded = files.upload()\n\nif not uploaded:\n  UPLOADED_FILE = ''\nelif len(uploaded) == 1:\n  UPLOADED_FILE = list(uploaded.keys())[0]\nelse:\n  raise AssertionError('Please upload one image at a time')\n\nprint(UPLOADED_FILE)","metadata":{"id":"m5jPDsEA5Kub","cellView":"form","outputId":"5e804e79-e418-47e5-dc96-b2079159e722"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Or download random samples form COCO test set (Karpathy et al. split)\n\nIMAGE_NAME = '354533'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']\n\nname_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\nimages_path = os.path.join(os.path.dirname(current_directory), \"images\")\nos.makedirs(images_path, exist_ok=True)\nUPLOADED_FILE = os.path.join(images_path, name_)\n\nif not os.path.isfile(UPLOADED_FILE):\n  download_path = os.path.join(images_path, \"images.zip\")\n  downloader.download_file(\"1BwJeBME-dpwcCT8IXYeWz7uaPkbexjNB\", download_path)\n\n  !unzip {download_path} -d {images_path}\n\n","metadata":{"id":"pohtQ8AfWNk_","cellView":"form","outputId":"5116f31c-1e23-4e2a-9e05-5d8ed1ab835e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conceptual captions examples:\nhttps://drive.google.com/file/d/1mzH3b0LQrGEWjEva4hI6HE_fIYRIgtBT/view?usp=sharing","metadata":{"id":"XyVkuZ07llSC"}},{"cell_type":"code","source":"#@title Inference\nuse_beam_search = False #@param {type:\"boolean\"}  \n\nimage = io.imread(UPLOADED_FILE)\npil_image = PIL.Image.fromarray(image)\n#pil_img = Image(filename=UPLOADED_FILE)\ndisplay(pil_image)\n\nimage = preprocess(pil_image).unsqueeze(0).to(device)\nwith torch.no_grad():\n    # if type(model) is ClipCaptionE2E:\n    #     prefix_embed = model.forward_image(image)\n    # else:\n    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\nif use_beam_search:\n    generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\nelse:\n    generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n\n\nprint('\\n')\nprint(generated_text_prefix)","metadata":{"id":"rRmcYnEfSMc_","cellView":"form","outputId":"9f169ac4-ae39-4e51-b8ca-7955a212813e"},"outputs":[],"execution_count":null}]}